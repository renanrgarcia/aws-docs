# S3 Baseline Performance

- Amazon S3 automatically scales to high request rates, latency 100-200 ms
- Your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.
- There are no limits to the number of prefixes in a bucket.
- Example (object path => prefix):
  - bucket/folder1/sub1/file => /folder1/sub1/
  - bucket/folder1/sub2/file => /folder1/sub2/
  - bucket/1/file => /1/
  - bucket/2/file => /2/
- If you spread reads across all four prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD

## Multi-part Upload

- recommended for files > 100MB, must use for files > 5GB
- Can help parallelize uploads (speed up transfers)
- Big file => split into parts => upload parts in parallel => S3 assembles the parts and creates the object

## S3 Transfer Acceleration

- Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region
- Compatible with multi-part upload
- Edge location: data center that is part of the AWS global network of edge locations
  - Good for long distances and high network congestion
- File in USA =(Fast - public internet)> edge location in USA =(Fast - AWS private network)> S3 bucket in Australia

## S3 Bytes-Range Fetches

- Parallelize GETs by requesting specific byte ranges
- Better resilience in case of failures
- Can be used to speed up downloads
- Example:

  - File in S3 (Broken into n parts) => Client requests byte range 1, byte range 2, ..., byte range n in parallel => Client reassembles the parts

- Can be used to retrieve only partial data (for example the head of a file)
- Example: retrieve only the first 1MB of a 10GB file
